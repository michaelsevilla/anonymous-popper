\relax 
\citation{sevilla:sc15-mantle,ren:sc2014-indexfs}
\citation{bent:login16-hpc-trends}
\citation{thusoo:sigmod2010-facebook-infrastructure}
\citation{mckusick:acm2010-gfs-evolution}
\citation{roselli:atec2000-FS-workloads}
\citation{roselli:atec2000-FS-workloads,abad:techreport2012-fstrace,abad:ucc2012-mimesis,alam:pdsw2011-metadata-scaling,weil:osdi2006-ceph}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Administrators can assign consistency and fault tolerance policies to subtrees to get the benefits of some of the state-of-the-art HPC architectures. }}{1}}
\newlabel{fig:subtree-policies}{{1}{1}}
\citation{GIGA+}
\citation{IndexFS,CephFS}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Metadata Load Balancing}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Relaxing POSIX}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Create-heavy workloads (untar) incurr the highest disk, network, and CPU utilization because the metadata server is managing consistency and fault tolerance.}}{3}}
\newlabel{fig:creates-motivation}{{2}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}POSIX Overheads}{3}}
\citation{}
\citation{}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Fault Tolerance}{4}}
\newlabel{sec:fault-tolerance}{{3.1}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Strong Consistency}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Consistency Overhead}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces CephFS uses a journal to stage updates and tracks dirty metadata in the collective memory of the MDSs. Each MDS maintains its own journal, which is broken up into 4MB segments. These segments are pushed into RADOS and deleted when that particular segment is trimmed from the end of the log. In addition to journal segments, RADOS also stores per-directory objects. }}{5}}
\newlabel{fig:journal}{{3}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Performance improves with larger journal segments because the metadata server spends less time flushing the journal }}{5}}
\newlabel{fig:throughput-journal}{{4}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Journalling metadata updates has a bigger overhead than maintaining the inode cache. For create-heavy workloads the inode cache offers no performance benefits. }}{5}}
\newlabel{fig:throughput-cache-journal}{{5}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The inode cache improves metadata read performance but for our create-heavy workload it is only an overhead. Most of the time maintaining the cache is spent evicting and adding inodes.}}{6}}
\newlabel{fig:inode-cache}{{6}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces When a client create stream is ``isolated" then lookups resolve locally but when a second client ``interferes" by creating in the same directory, the directory inode capability is revoked forcing all clients to centralize lookups at the metadata server. }}{6}}
\newlabel{fig:throughput-droplease}{{7}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces An underloaded metadata server adaquately services conflicting clients; the create speeds are similar while the interferring operations are limited by the cost of RPCs.}}{7}}
\newlabel{fig:exp0-underloaded}{{8}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Scaling clients shows increased variability when another client interferes; zooming in on runs with 7 clients we see that different types of interferring operations have different effects on performance variability and predictability. }}{7}}
\newlabel{fig:runtime-consistency-scale}{{9}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Methodology: Decoupled Namespaces}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Cudele's Mechanisms}{7}}
\newlabel{sec:cudeles-mechanisms}{{4.1}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Applications can decouple the namespace, write updates to a local journal, and delay metadata updates. Table\nobreakspace  {}1\hbox {} shows how these phases (represented by the arrows) can be combined to provide weaker consistency or fault tolerance semantics. }}{8}}
\newlabel{fig:decouple}{{10}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Cudele lets future programmers explore the spectrum of consistency and fault tolerance semantics shown below.}}{8}}
\newlabel{table:spectrum}{{1}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Setting Policies with Cudele}{8}}
\newlabel{sec:setting-policies-with-cudele}{{4.2}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Cudele as a Programmable Storage System}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}No Changes: Turning Knobs}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}External Library: Reads/Writes Journal Events}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Storage System Changes: Volatile Merge}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Implementation}{9}}
\newlabel{sec:implementation}{{5}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}RPCs (Existing CephFS Implementation)}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Stream (Existing Implementation)}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Create}{9}}
\bibdata{paper}
\bibcite{abad:techreport2012-fstrace}{{1}{}{{}}{{}}}
\bibcite{abad:ucc2012-mimesis}{{2}{}{{}}{{}}}
\bibcite{alam:pdsw2011-metadata-scaling}{{3}{}{{}}{{}}}
\bibcite{bent:login16-hpc-trends}{{4}{}{{}}{{}}}
\bibcite{mckusick:acm2010-gfs-evolution}{{5}{}{{}}{{}}}
\bibcite{ren:sc2014-indexfs}{{6}{}{{}}{{}}}
\bibcite{roselli:atec2000-FS-workloads}{{7}{}{{}}{{}}}
\bibcite{sevilla:sc15-mantle}{{8}{}{{}}{{}}}
\bibcite{thusoo:sigmod2010-facebook-infrastructure}{{9}{}{{}}{{}}}
\bibcite{weil:osdi2006-ceph}{{10}{}{{}}{{}}}
\bibstyle{plain}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Compared to the \texttt  {create} phase, saving and persisting updates (\texttt  {create+save} and \texttt  {create+save+persist}) experience only a 4.79\(\times \) and 8.66\(\times \) slowdown, in the worst case for 100K files. In contrast, maintaining \texttt  {global} consistency is 905.70\(\times \) slower. The disadvantage of decoupling the namespace is the merge phase where updates are applied to the metadata store (\texttt  {create+apply}, resulting in a 905.70\(\times \) slowdown for 100K files.}}{10}}
\newlabel{fig:global-v-decoupled}{{11}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Volatile Apply (v.apply)}{10}}
\newlabel{sec:vapply}{{5.4}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Save}{10}}
\newlabel{sec:save}{{5.5}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Persist}{10}}
\newlabel{sec:persist}{{5.6}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Results}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Microbenchmarks}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}Per phase latencies}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Journaling Overhead}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Macrobenchmarks}{10}}
\newlabel{sigplanconf@finalpage}{{6.3}{10}}
