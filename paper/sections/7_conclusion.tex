%\section{Discussion}
%
%% Could we have implemented this on something other than CephFS?
%One question that comes up often in our work with Ceph is: ``Why do we always
%choose Ceph and can we implement this on another storage system?" Ceph is
%popular, robust, and open source -- getting it merged into mainline has a
%better chance of getting used than if we had built the system `clean-slate'
%from the ground up. Using Ceph, we can also leverage its robustness using an
%approach called ``programmability".
%
%% How awesome programmability is
%``Programmability" is a way of designing storage systems that encourages code
%re-use and composition.  A programmable storage system exposes internal
%subsystem as building blocks for higher level services. This `dirty-slate'
%approach limits redundant code and leverages the robustness of the underlying
%storage system. CudeleFS uses this approach and re-uses some of the building
%blocks from the Malacology programmable storage system~\cite{sevilla:eurosys17}
%to great success and requires only:
%
%\begin{itemize}
%
%  \item 354 lines of library code
%
%  \item 219 lines of non-destructive metadata server code, which is not used
%  unless it is turned on
%
%  \item 4 lines of destructive client/server code to check whether a namespace
%  is decoupled
%
%\end{itemize}

\section{Conclusion}

Relaxing consistency and durability semantics in the file system is a
double-edged sword. While it achieves better performance and scalability, it
alienates applications that rely on strong consistency and durability. CudeleFS
lets users assign consistency and durability guarantees to subtrees in
the global namespace, giving users a wide range of policies and
optimizations that can be custom fit to the application. Using CudeleFS, we
compare related work on the same file system and conclude that strong
consistency can cause a 104\(\times\) slow down while merging updates and
maintaining durability only have between a \(7-10\times\) slow down.

% Implied Namespaces

% Intermediate Update Bursts

% See if this helps load balancing

% executing mechanisms in parallel
